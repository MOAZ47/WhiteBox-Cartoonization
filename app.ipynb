{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2c0248d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os, io, warnings\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display, Image as im\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09c21900",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3804fa29",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#D3D3D3\"><h1><b><center>White-box Cartoon Representations</center></b></h1></div><br>\n",
    "This is a demo application for the AIDI 1003 Capstone project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc4f83ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, stride, padding, padding_mode):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=kernel_size, stride=stride, padding=padding, padding_mode=padding_mode),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            nn.Conv2d(channels, channels, kernel_size=kernel_size, stride=stride, padding=padding, padding_mode=padding_mode),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Elementwise Sum (ES)\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_channels, num_features=32, num_residuals=4):\n",
    "        super().__init__()\n",
    "        self.padding_mode = \"zeros\"\n",
    "\n",
    "        self.initial_down = nn.Sequential(\n",
    "            #k7n32s1\n",
    "            nn.Conv2d(img_channels, num_features, kernel_size=7, stride=1, padding=3, padding_mode=self.padding_mode),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        #Down-convolution\n",
    "        self.down1 = nn.Sequential(\n",
    "            #k3n32s2   256, 256, 32\n",
    "            nn.Conv2d(num_features, num_features, kernel_size=3, stride=2, padding=1, padding_mode=self.padding_mode),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "\n",
    "            #k3n64s1   128, 128, 64\n",
    "            nn.Conv2d(num_features, num_features*2, kernel_size=3, stride=1, padding=1, padding_mode=self.padding_mode),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.down2 = nn.Sequential(\n",
    "            #k3n64s2\n",
    "            nn.Conv2d(num_features*2, num_features*2, kernel_size=3, stride=2, padding=1, padding_mode=self.padding_mode),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "\n",
    "            #k3n128s1  64, 64, 128\n",
    "            nn.Conv2d(num_features*2, num_features*4, kernel_size=3, stride=1, padding=1, padding_mode=self.padding_mode),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        #Bottleneck: 4 residual blocks => 4 times [K3n128s1]  64, 64, 128\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(num_features*4, kernel_size=3, stride=1, padding=1, padding_mode=self.padding_mode) for _ in range(num_residuals)]\n",
    "        )\n",
    "\n",
    "        #Up-convolution\n",
    "        self.up1 = nn.Sequential(\n",
    "            #k3n128s1   64, 64, 128\n",
    "            nn.Conv2d(num_features*4, num_features*2, kernel_size=3, stride=1, padding=1, padding_mode=self.padding_mode),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.up2 = nn.Sequential(\n",
    "            #k3n64s1\n",
    "            nn.Conv2d(num_features*2, num_features*2, kernel_size=3, stride=1, padding=1, padding_mode=self.padding_mode),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            #k3n64s1 (should be k3n32s1?)  128, 128, 64\n",
    "            nn.Conv2d(num_features*2, num_features, kernel_size=3, stride=1, padding=1, padding_mode=self.padding_mode),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.last = nn.Sequential(\n",
    "            #k3n32s1\n",
    "            nn.Conv2d(num_features, num_features, kernel_size=3, stride=1, padding=1, padding_mode=self.padding_mode),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            #k7n3s1   256, 256, 32\n",
    "            nn.Conv2d(num_features, img_channels, kernel_size=7, stride=1, padding=3, padding_mode=self.padding_mode)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.initial_down(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x = self.down2(x2)\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.up1(x)\n",
    "        #Resize Bilinear\n",
    "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners = False)\n",
    "        x = self.up2(x + x2) \n",
    "        #Resize Bilinear\n",
    "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners = False)\n",
    "        x = self.last(x + x1)\n",
    "        #TanH\n",
    "        return torch.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02e7aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, lr, path):\n",
    "    #print(\"=> Loading checkpoint\")\n",
    "    if (os.path.isfile(path)):\n",
    "        checkpoint = torch.load(path, map_location = DEVICE)\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "        # If we don't do this then it will just have learning rate of old checkpoint\n",
    "        # and it will lead to many hours of debugging \\:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "        #print(\"Checkpoint file \" + str(path) + \" loaded.\")\n",
    "        loaded = True\n",
    "    else:\n",
    "        print(\"Checkpoint file \" + str(path) + \" not found. Not loading checkpoint.\")\n",
    "        loaded = False\n",
    "    return model, optimizer, loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bd65e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_crop(image):\n",
    "  target_height = 256\n",
    "  target_width = 256\n",
    "  w, h = image.size\n",
    "\n",
    "  if h and w == 256:\n",
    "    print(\"No resizing needed\")\n",
    "    return image\n",
    "  elif (h!=256) or (w!= 256):\n",
    "    diff_height = abs(target_height - h)\n",
    "    if diff_height > 256:\n",
    "      h = h - diff_height\n",
    "    elif diff_height < 256:\n",
    "      h = diff_height + h\n",
    "    else:\n",
    "      h = diff_height\n",
    "    \n",
    "    diff_width = abs(target_width - w)\n",
    "    if diff_width > 256:\n",
    "      w = w - diff_width\n",
    "    elif diff_width < 256:\n",
    "      w = diff_width + w\n",
    "    else:\n",
    "      w = diff_width\n",
    "  image = image.resize((w, h), Image.ANTIALIAS)\n",
    "  print(\"Done resizing\")\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7cbec110",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_greet = widgets.Label('Click on the beow button to select the Image you want to animate')\n",
    "\n",
    "btn_upload = widgets.FileUpload()\n",
    "\n",
    "out_pl = widgets.Output()\n",
    "\n",
    "lbl_pred = widgets.Label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "046c59c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cpu'\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "trained_network_name = \"201_last_gen.pth.tar\"\n",
    "\n",
    "gen = Generator(img_channels=3).to(DEVICE)\n",
    "opt_gen = torch.optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "load_checkpoint(gen, opt_gen, LEARNING_RATE, path = trained_network_name)\n",
    "\n",
    "def on_data_change(change):\n",
    "    \n",
    "    #img_test = image.load_img(btn_upload.data[-1])\n",
    "    #img = PIL.Image.open(btn_upload.data[-1])\n",
    "    #img_test = image.img_to_array(img)\n",
    "    \n",
    "    for name, file_info in btn_upload.value.items():\n",
    "        test_image = Image.open(io.BytesIO(file_info['content'])).convert(\"RGB\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #test_image = np.asarray(test_image)\n",
    "        test_image = resize_crop(test_image)\n",
    "        test_image = transforms.ToTensor()(test_image).to(DEVICE).unsqueeze_(0)\n",
    "        y_fake = gen(test_image)\n",
    "    \n",
    "    out_pl.clear_output()\n",
    "    \n",
    "    #with out_pl: display(img_test.to_thumb(128,128))\n",
    "    with out_pl:\n",
    "        fig = plt.figure(figsize=(10, 7))\n",
    "        \n",
    "        fig.add_subplot(1, 2, 1)\n",
    "        og_image = test_image.cpu().squeeze_(0).permute(1, 2, 0).numpy()\n",
    "        og_image = np.clip(og_image, 0, 1)\n",
    "        plt.imshow(og_image)\n",
    "        plt.axis('off')\n",
    "        #plt.show()\n",
    "        fig.add_subplot(1, 2, 2)\n",
    "        fake_image = y_fake.cpu().squeeze_(0).permute(1, 2, 0).numpy()\n",
    "        fake_image = np.clip(fake_image, 0, 1)\n",
    "        plt.imshow(fake_image)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01775a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_upload.observe(on_data_change, names=['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "59c4706e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305c46f120a1464795c0f630e43a1330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Click on the beow button to select the Image you want to animate')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526f50b1d42d403998222dcda6250775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711cb3d3a4ba4fd2be23a45f588e7e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(lbl_greet, btn_upload, out_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc434c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c363abe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
